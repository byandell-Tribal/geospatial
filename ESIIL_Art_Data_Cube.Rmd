---
title: "Pre_Hack_Geospatial"
author: "Brian Yandell"
date: "2023-11-13"
output: html_document
params:
  dontrun: TRUE
  localname: "Boulder County"
  localarea: "boulder, co"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Code below was edited from [https://github.com/CU-ESIIL/hackathon2023_datacube/docs/code_for_building_cube/stac_mount_save.qmd](https://raw.githubusercontent.com/CU-ESIIL/hackathon2023_datacube/main/docs/code_for_building_cube/stac_mount_save.qmd) as the basis for
[Ty Tuff's The art of making a datacube](https://cu-esiil.github.io/hackathon2023_datacube/code_for_building_cube/stac_mount_save/).

If Rmarkdown parameter `params$dontrun` is `r TRUE` then knitting this document will show the remaining code without running.

```{r}
if(params$dontrun) {
  warning("Showing rest of code without running")
  knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
}
```

# Libraries for Data Cube

```{r load_libraries, cache=FALSE, message=FALSE, warning=FALSE, collapse=TRUE}
#library(Rcpp)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(tidyr)
library(glue)
library(colorspace)
library(reshape2)

library(sf)
library(terra)
library(stars)

library(gdalcubes)
library(rstac)
library(osmdata)
library(osmdata)
library(tidyterra)
library(raster) # used below once; is there a `stars` package option?

# Other packages not directly used below
#library(gdalUtils)
#library(rgdal) removed from CRAN
#library(geos)
#library(landsat)
```

```{r gdalcubes options}
gdalcubes::gdalcubes_options(parallel = 8)

sf::sf_extSoftVersion()
gdalcubes::gdalcubes_gdal_has_geos()
```

There are likely multiple resources to understand these packages.

- [Spatial Data Science by Edzer Pebesma and Roger Bivand](https://r-spatial.org/book/)
- <https://github.com/appelmar/gdalcubes>
- <https://r-spatial.org/>
  + <https://github.com/r-spatial/stars>
  + <https://github.com/r-spatial/sf>
- <https://rspatial.org/>
  + <https://github.com/rspatial/terra>
  + <https://github.com/rspatial/raster>
- [Comparison of terra and stars packages by Chris Brown](https://www.seascapemodels.org/rstats/2021/06/01/STARS.html)
- [Marius Appel - Creating and Analyzing Multi-Variable Earth Observation Data Cubes in R (part 1)](https://www.youtube.com/watch?v=kE-se6zg6HE) (2 hours)
- [Stack Overflow: stack geotiff with stars 'along' when 'band' dimension contains band + time information](https://stackoverflow.com/questions/75249639/stack-geotiff-with-stars-along-when-band-dimension-contains-band-time-info)

See also the documentation for packages such as `sf`, `terra` and `gdlacubes`, which
seem to be the main packages in use now.
Not sure where `raster` fits, but it is called once below.
Presumably packages `geos` and `landsat` are included as they serve their respective data repositories, although not used below.

# Mounting data

A void-filled Digital Elevation Model (DEM) is a comprehensive topographical representation where any missing data points, known as voids, have been filled in. These voids can occur due to various reasons, such as clouds or technical errors during data collection. In a void-filled DEM, these gaps are interpolated or estimated using the surrounding data to create a continuous, seamless surface model. This process enhances the utility and accuracy of the DEM for hydrological modeling, terrain analysis, and other geographical applications. The [HydroSHEDS website](https://www.hydrosheds.org/hydrosheds-core-downloads) provides access to high-quality, void-filled DEM datasets like the DEM_continuous_CONUS_15s, which users can download and easily integrate into spatial analysis workflows using tools such as 'terra' in R, allowing for sophisticated environmental and geographical research and planning.

```{r DEM, cache=FALSE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()  

# Create a string with the file path using glue, then download and read the DEM file as a raster object

DEM_continuous_CONUS_15s <- 
  glue::glue(
    # magic remote VSI connection
    "/vsizip/vsicurl/", 
    # copied link to download location
    "https://data.hydrosheds.org/file/hydrosheds-v1-dem/hyd_na_dem_15s.zip",
    # path inside zip file (may have to pre-open to find)
    "/hyd_na_dem_15s.tif") |>
  terra::rast()

# The 'glue' function constructs the file path string, which is then passed to 'terra::rast()' to read the DEM file into R as a raster layer. '/vsizip/vsicurl/' is a special GDAL virtual file system syntax that allows reading directly from a zipped file on a remote server.

# Record end time and calculate the time difference
b <- Sys.time()  
difftime(b, a) 

# The resulting raster object is stored in 'DEM_continuous_CONUS_15s', which now contains the void-filled DEM data ready for use

# Print out the details of the 'DEM_continuous_CONUS_15s' raster object
DEM_continuous_CONUS_15s  

# output is a SpatRaster, which is the object type associated with the 'terra' package. 
```

## Continuous DEM for North America

```{r DEM_plot, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

ggplot2::ggplot() +
  tidyterra::geom_spatraster(data=DEM_continuous_CONUS_15s) +
  ggthemes::theme_tufte()

b <- Sys.time()
difftime(b, a)
```

### Calculate Slope from that DEM

```{r slope, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
SLOPE_continuous_CONUS_15s <- terra::terrain(DEM_continuous_CONUS_15s, "slope") 

SLOPE_continuous_CONUS_15s
```

```{r SLOPE_plot, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

ggplot2::ggplot() +
  tidyterra::geom_spatraster(data=SLOPE_continuous_CONUS_15s) +
  ggthemes::theme_tufte()

b <- Sys.time()
difftime(b, a)
```

### Calculate aspect from DEM

```{r aspect, cache=FALSE, warning=FALSE, message=FALSE, collapse=TRUE}
(ASPECT_continuous_CONUS_15s <- terra::terrain(DEM_continuous_CONUS_15s, "aspect")) 
```

```{r ASPECT_plot, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

ggplot2::ggplot() +
  tidyterra::geom_spatraster(data=ASPECT_continuous_CONUS_15s) +
  ggthemes::theme_tufte()

b <- Sys.time()
difftime(b, a)
```

### Create a cube from those layers!

```{r mini_stack, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
(mini_stack <- c(DEM_continuous_CONUS_15s,
                SLOPE_continuous_CONUS_15s,
                ASPECT_continuous_CONUS_15s))
```

### Reproject and return bounding box coordinates for Area of Interest

Data are stored in different coordinating systems, which makes it important
to transform between them. Some common ones:

- [EPSG:4326](https://epsg.io/4326): WGS84 = World Geodetic System 1984
- [EPSG:32618](https://epsg.io/32618): WGS84 for UTM zone 18N (North America)
- [EPSG:32730](https://epsg.io/32720): WGS84 for UTM zone 20S (South America)

```{r DEM_bounding_box, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Transform the filtered geometry to EPSG:4326 and store its bounding box
# Record start time
a <- Sys.time()

DEM_continuous_CONUS_15s |>
  stars::st_as_stars() |> 
  sf::st_transform("EPSG:4326") |>
  sf::st_bbox() -> bbox_4326

DEM_continuous_CONUS_15s |>
  stars::st_as_stars() |> 
  sf::st_transform("EPSG:32618") |>
  sf::st_bbox() -> bbox_32618

b <- Sys.time()
difftime(b, a)
```

## Focus on `r params$localname`

Get a polygon for `r params$localname`, reproject, and return bounding box. This is so I can make a smaller search in the stac catalog. 

```{r local_bounding_box, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
local_area <- getbb(params$localarea, format_out="sf_polygon")

local_area$multipolygon |> 
  sf::st_transform(crs = 4326) |>
  sf::st_bbox() -> bbox_4326_local

local_area$multipolygon |> 
  sf::st_transform(crs = 32720) |>
  sf::st_bbox() -> bbox_32720_local
```

Get a polygon for the United States and crop it to be the same size as the DEM above. 

```{r conus_bounding_box, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
aoi <- osmdata::getbb("United States", format_out="sf_polygon")

conus <- aoi$multipolygon |>
  sf::st_crop(bbox_4326)

ggplot2::ggplot(data=conus) +
  ggplot2::geom_sf()
```

# Search the Stac catalog

STAC, or SpatioTemporal Asset Catalog, is an open-source specification designed to standardize the way geospatial data is indexed and discovered. Developed by Element 84 among others, it facilitates better interoperability and sharing of geospatial assets by providing a common language for describing them. STAC's flexible design allows for easy cataloging of data, making it simpler for individuals and systems to search and retrieve geospatial information. By effectively organizing data about the Earth's spatial and temporal characteristics, STAC enables users to harness the full power of the cloud and modern data processing technologies, optimizing the way we access and analyze environmental data on a global scale.

```{r basic_stac_request, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
rstac::stac("https://earth-search.aws.element84.com/v1") |>
  rstac::get_request()
```

Element 84's Earth Search is a STAC compliant search and discovery API that offers users access to a vast collection of geospatial open datasets hosted on AWS. It serves as a centralized search catalog providing standardized metadata for these open datasets, designed to be freely used and integrated into various applications. Alongside the API, Element 84 also provides a web application named Earth Search Console, which is map-centric and allows users to explore and visualize the data contained within the Earth Search API's catalog. This suite of tools is part of Element 84's initiative to make geospatial data more accessible and actionable for a wide range of users and applications.

```{r stac_collection_list, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
gdalcubes::collection_formats()
```

## Build stac collection by aiming camera at landscape

Creating a STAC collection is akin to a photographer framing a shot; the landscape is rich with diverse data, mirroring a scene bustling with potential subjects, colors, and light. Just as a photographer selects a portion of the vista to capture, focusing on elements that will compose a compelling image, a data scientist must similarly navigate the vast data terrain. They must 'point their camera' judiciously, ensuring that the 'frame' encapsulates the precise data needed. This careful selection is crucial, as it determines the relevance and quality of the data collection, much like the photographer's choice dictates the story a photograph will tell.

```{r build_collection, cache=FALSE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

# Initialize STAC connection
s <- rstac::stac("https://earth-search.aws.element84.com/v0")


# Search for Sentinel-2 images within specified bounding box and date range
#22 Million items
items <- s |>
  rstac::stac_search(
    collections = "sentinel-s2-l2a-cogs",
    bbox = c(bbox_4326_local["xmin"], 
             bbox_4326_local["ymin"],
             bbox_4326_local["xmax"], 
             bbox_4326_local["ymax"]), 
    datetime = "2021-05-15/2021-05-16") |>
  rstac::post_request() |>
  rstac::items_fetch(progress = FALSE)

# Print number of found items
length(items$features)

# Prepare the assets for analysis
assets <- c("B01", "B02", "B03", "B04", "B05", "B06", "B07", 
           "B08", "B8A", "B09", "B11", "B12", "SCL")
s2_collection <- sf::stac_image_collection(
  items$features, asset_names = assets,
  # all images with less than 20% clouds
  property_filter = function(x) {x[["eo:cloud_cover"]] < 20})

b <- Sys.time()
difftime(b, a)

# Display the image collection
s2_collection
```

### Set up camera and film

The camera through which the data scientist frames the shot is multifaceted, akin to the tools and processes they employ. The camera's film, analogous to the data cube, defines the resolution and dimensions of the captured data, shaping how the final dataset will be utilized. The lens and its settings—focus, aperture, and exposure—determine the clarity, depth, and breadth of the captured information, much like the algorithms and parameters set by the data scientist dictate the granularity and scope of the data cube. The flash, like data enhancement techniques, can illuminate hidden details, ensuring that the data cube, the final product, is as informative and accurate as the landscape it represents.

```{r set_view_window, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

# Define a specific view on the satellite image collection
v <- gdalcubes::cube_view(
    srs = "EPSG:32720", #this is harder than expected. 
    dx = 100, 
    dy = 100, 
    dt = "P1M", 
    aggregation = "median", 
    resampling = "near",
    extent = list(
        t0 = "2021-05-15", 
        t1 = "2021-05-16",
        left = bbox_32720_local[1], 
        right = bbox_32720_local[2],
        top = bbox_32720_local[4], 
        bottom = bbox_32720_local[3]))

b <- Sys.time()
difftime(b, a)

# Display the defined view
v
```

### Take picture

#### Raster style

This code appears to come directly from <https://github.com/appelmar/gdalcubes/README.md>,
including the call to `stack()` in package `raster`.
Not sure if there is another routine in package `stars`, say, that supersedes this.

```{r make_raster_cube, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

x <- s2_collection |>
  gdalcubes::raster_cube(v) |>
  gdalcubes::select_bands(c( "B04", "B05"))  |>
  gdalcubes::apply_pixel(c("(B05-B04)/(B05+B04)"), names="NDVI") |>
  gdalcubes::write_tif() |>
  raster::stack()

b <- Sys.time()
difftime(b, a)

x
```

#### STARS style

```{r make_STARS_cube, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

y <- s2_collection |>
  gdalcubes::raster_cube(v) |>
  gdalcubes::select_bands(c("B04","B05"))  |>
  gdalcubes::apply_pixel(c("(B05-B04)/(B05+B04)"), names="NDVI") |>
  stars::st_as_stars()

b <- Sys.time()
difftime(b, a)

y
```

### Extract data 

```{r extract_spectra, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
# Record start time
a <- Sys.time()

x <- s2_collection |>
  gdalcubes::raster_cube(v) |>
  gdalcubes::select_bands(c("B01", "B02", "B03", "B04", 
                   "B05", "B06", "B07", "B08", 
                   "B8A", "B09", "B11", "B12")) |>
  gdalcubes::extract_geom(local_area$multipolygon) |>
  dplyr::rename(
#        "time" = "time",
        "443" = "B01",
        "490" = "B02",
        "560" = "B03",
        "665" = "B04",
        "705" = "B05",
        "740" = "B06",
        "783" = "B07",
        "842" = "B08",
        "865" = "B8A",
        "940" = "B09",
        "1610" = "B11",
        "2190" = "B12")

b <- Sys.time()
difftime(b, a)

head(x)
```

# Make a timeseries

```{r time_series_cube, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}

# Record start time
a <- Sys.time()

items <- s |>
  rstac::stac_search(
    collections = "sentinel-s2-l2a-cogs",
    bbox = c(-105.694362,	39.912886,	-105.052774,	40.262785),
    datetime = "2020-01-01/2022-12-31",
    limit = 500) |> 
  rstac::post_request() 

S2.mask <- gdal_cubes::image_mask("SCL", values=c(3,8,9))

col <- gdalcubes::stac_image_collection(
  items$features, asset_names = assets, 
  property_filter = function(x) {x[["eo:cloud_cover"]] < 30})

v <- gdalcubes::cube_view(
  srs = "EPSG:4326",  extent = list(t0 = "2020-01-01", t1 = "2022-12-31",
  left = -105.694362, right = -105.052774,  top = 40.262785, bottom = 39.912886),
  dx = 0.001, dy = 0.001, dt = "P1M", aggregation = "median", resampling = "bilinear")

ndvi.col = function(n) {
  rev(colorspace::sequential_hcl(n, "Green-Yellow"))
}
z <- gdalcubes::raster_cube(col, v, mask = S2.mask) |>
  gdalcubes::select_bands(c("B04", "B08")) |>
  gdalcubes::apply_pixel("(B08-B04)/(B08+B04)", "NDVI") |>
  gdalcubes::animate(col = ndvi.col, zlim=c(-0.2,1), key.pos = 1,
                     save_as = "anim.gif", fps = 4)

b <- Sys.time()
difftime(b, a)

z
```

# Save Data Cubes to Local Storage

There are occasions when we need to manipulate data cubes using other software. For such purposes, we can save data cubes to our local disk as individual netCDF files or as a series of GeoTIFF files. In the case of the latter, each temporal segment of the cube is saved as a separate (multiband) GeoTIFF file.

Both netCDF and GeoTIFF formats allow for file size reduction through compression and data packing. This process involves transforming double precision numbers into smaller integer values using a scale and offset, which can be particularly useful for managing disk space (for more details, refer to the ?write_ncdf and ?write_tif documentation).

```{r, eval=FALSE, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
gdalcubes::gdalcubes_options(ncdf_compression_level = 1)
gdalcubes::write_ncdf(cube, file.path("~/Desktop", basename(tempfile(fileext = ".nc"))))
gdalcubes::gdalcubes_options(ncdf_compression_level = 0)
```

`write_tif()` and `write_ncdf()` both return the path(s) to created file(s) as a character vector.

```{r, eval=FALSE, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
items_2020 <- s |>
  rstac::stac_search(
    collections = "sentinel-s2-l2a-cogs",
    bbox = c(-105.694362,	39.912886,	-105.052774,	40.262785),
    datetime = "2020-05-01/2020-06-30") |> 
  rstac::post_request() 

items_2021 <- s |>
  rstac::stac_search(
    collections = "sentinel-s2-l2a-cogs",
    bbox = c(-105.694362,	39.912886,	-105.052774,	40.262785),
    datetime = "2021-05-01/2021-06-30") |> 
  rstac::post_request() 


col_2020 <- rstac::stac_image_collection(items_2020$features, asset_names = assets)
col_2021 <- rstac::stac_image_collection(items_2021$features, asset_names = assets)

v_2020 <- gdalcubes::cube_view(
  srs = "EPSG:32720",  extent = list(t0 = "2020-05-01", t1 = "2020-06-30",
  left = bbox_32720_local["xmin"], right = bbox_32720_local["xmax"],
  top = bbox_32720_local["ymax"], bottom = bbox_32720_local["ymin"]),
  dx = 100, dy = 100, dt = "P1D", aggregation = "median", resampling = "bilinear")

v_2021 <- gdalcubes::cube_view(
  v_2020, extent = list(t0 = "2021-05-01", t1 = "2021-06-30"))

max_ndvi_mosaic <- function(col, v) {
  gdalcubes::raster_cube(col, v) |>
  gdalcubes::select_bands(c("B04", "B08")) |>
  gdalcubes::apply_pixel(c("(B08-B04)/(B08+B04)"), names="NDVI") |>
  gdalcubes::reduce_time("max(NDVI)")
}

(maxndvi_2020 <- max_ndvi_mosaic(col_2020, v_2020))
(maxndvi_2021 <- max_ndvi_mosaic(col_2021, v_2021))

difference <- maxndvi_2021 - maxndvi_2020
difference[difference > -0.15] <- NA
names(difference) <- "Difference of max NDVI (2020 - 2019)"
```

#### GloRiC in shapefile format

```{r, eval=FALSE, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
flood_polygon_data3 <-
  glue::glue(
    "/vsizip/vsicurl/",
    "https://data.hydrosheds.org/file/hydrosheds-associated/gloric/GloRiC_v10_shapefile.zip",
    "/GloRiC_v10_shapefile/GloRiC_v10.shp") |>
  sf::st_read() |>
  sf::st_as_sf(coords = c("lon","lat"))

flood_polygon_data3
```

#### GloRiC in geodatabase format

```{r, eval=FALSE, cache=TRUE, warning=FALSE, message=FALSE, collapse=TRUE}
#st_read("/Users/ty/Downloads/GloRiC_v10_geodatabase/GloRiC_v10.gdb")

flood_polygon_data3 <-
  glue::glue(
    "/vsizip/vsicurl/",
    "https://data.hydrosheds.org/file/hydrosheds-associated/gloric/GloRiC_v10_geodatabase.zip",
    "/GloRiC_v10_geodatabase/GloRiC_v10.gdb") |>
  sf::st_read() |>
  sf::st_as_sf(coords = c("lon","lat"))

flood_polygon_data3
```
