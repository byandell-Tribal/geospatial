---
title: "student edition"
format: gfm
---

# Make map

The provided R script outlines a comprehensive workflow for mapping and analyzing geographic data using various spatial functions and visualization tools in R. This process involves loading, processing, and mapping data with a focus on urban planning and environmental analysis, particularly using the example of redlining data for the city of Denver.

Overview and Usage
Setup and Dependencies: The script starts by loading necessary R packages like sf for handling spatial data, ggplot2 for plotting, and others like gdalcubes and dplyr for data manipulation and analysis. It also includes installation of custom libraries like basemapR via devtools.

Data Loading (load_city_redlining_data): This function retrieves redlining data from a specified URL, filters it by city, and reads it into an sf object, which is a standard format for storing geometric location data and associated attributes in R.

Data Retrieval (get_places): This function is designed to fetch additional geographic data like roads and rivers based on the bounding box of the provided spatial layer. It demonstrates the integration of external data sources into spatial analysis workflows.

Data Visualization (plot_city_redlining and split_plot): These functions are used to create detailed maps that overlay various data layers such as roads, rivers, and residential zones based on redlining grades. The use of thematic mapping and faceting provides insights into the spatial distribution of urban features.

Advanced Spatial Analysis (process_and_plot_sf_layers): This function performs complex spatial operations like buffering and intersecting different layers to analyze the interactions between them. It exemplifies how to handle and visualize spatial relationships and dependencies.

```{r, collapse=TRUE}
library(sf)

library(dplyr)
library(rstac)
library(gdalcubes)
library(gdalUtils)
library(gdalcubes)
library(colorspace)
library(terra)

library(sf)
library(purrr)
library(dplyr)
library(gdalcubes)
```

```{r eval=FALSE}
install.packages("osmextract")
install.packages('ggthemes')
install.packages("rstac")
install.packages("tidyterra")
install.packages("glue")
install.packages("tidytext")
install.packages("ggwordcloud")
#install.packages("ggplot2", upgrade = TRUE)
install.packages("devtools")
install_github('Chrisjb/basemapR')

```

```{r}
library(glue)
library(tidytext)
library(osmextract)
library(ggthemes)
library(rstac)

library(ggwordcloud)

library(ggplot2)
library(tidyterra)

library(devtools)
library(basemapR)
```

```{r, collapse=TRUE}
# Function to load and filter redlining data by city
load_city_redlining_data <- function(city_name) {
  # URL to the GeoJSON data
  url <- "https://raw.githubusercontent.com/americanpanorama/mapping-inequality-census-crosswalk/main/MIv3Areas_2010TractCrosswalk.geojson"
  
  # Read the GeoJSON file into an sf object
  redlining_data <- read_sf(url)
  
  # Filter the data for the specified city and non-empty grades
  city_redline <- redlining_data %>%
    filter(city == city_name)
  
  # Return the filtered data
  return(city_redline)
}

```

```{r, cache=TRUE, warning=FALSE, message=FALSE}
# Load redlining data for Denver
denver_redlining <- load_city_redlining_data("Denver")
denver_redlining
```

```{r, collapse=TRUE}
get_places <- function(polygon_layer, type = "food") {
  # Check if the input is an sf object
  if (!inherits(polygon_layer, "sf")) {
    stop("The provided object is not an sf object.")
  }
  
  # Create a bounding box from the input sf object
  bbox_here <- st_bbox(polygon_layer) |>
    st_as_sfc()
  
  if (type == "roads") {
    my_layer <- "lines"
    my_query <- "SELECT * FROM lines WHERE (
                 highway IN ('motorway', 'trunk', 'primary', 'secondary', 'tertiary'))"
    title <- "Major roads"
  }
  
  if (type == "rivers") {
    my_layer <- "lines"
    my_query <- "SELECT * FROM lines WHERE (
                 waterway IN ('river'))"
    title <- "Major rivers"
  }
  
  # Use the bbox to get data with oe_get(), specifying the desired layer and a custom SQL query for fresh food places
  tryCatch({
    places <- oe_get(
      place = bbox_here,
      layer = my_layer,
      query = my_query,
      quiet = TRUE
    )
    
    places <- st_make_valid(places)
    
    # Crop the data to the bounding box
    cropped_places <- st_crop(places, bbox_here)
    
    # Plotting the cropped fresh food places
    plot <- ggplot(data = cropped_places) +
      geom_sf(fill="cornflowerblue", color="cornflowerblue") +
      ggtitle(title) +
      theme_tufte() +
      theme(legend.position = "none",  # Optionally hide the legend
            axis.text = element_blank(),     # Remove axis text
            axis.title = element_blank(),    # Remove axis titles
            axis.ticks = element_blank(),    # Remove axis ticks
            plot.background = element_rect(fill = "white", color = NA),  # Set the plot background to white
            panel.background = element_rect(fill = "white", color = NA),  # Set the panel background to white
            panel.grid.major = element_blank(),  # Remove major grid lines
            panel.grid.minor = element_blank()) 
    
    # Save the plot as a PNG file
    png_filename <- paste0(title, "_", Sys.Date(), ".png")
    ggsave(png_filename, plot, width = 10, height = 8, units = "in")
    
    # Return the cropped dataset
    return(cropped_places)
  }, error = function(e) {
    stop("Failed to retrieve or plot data: ", e$message)
  })
}
```


```{r, cache=TRUE, warning=FALSE, message=FALSE}
roads <- get_places(denver_redlining, type="roads")

rivers <- get_places(denver_redlining, type="rivers")
```

```{r, warning=FALSE, collapse=TRUE}
plot_city_redlining <- function(redlining_data, filename = "redlining_plot.png") {
  # Fetch additional geographic data based on redlining data
  roads <- get_places(redlining_data, type = "roads")
  rivers <- get_places(redlining_data, type = "rivers")
  
  # Filter residential zones with valid grades and where city survey is TRUE
  residential_zones <- redlining_data %>%
    filter(city_survey == TRUE & grade != "") 

  # Colors for the grades
  colors <- c("#76a865", "#7cb5bd", "#ffff00", "#d9838d")

  # Plot the data using ggplot2
  plot <- ggplot() +
    geom_sf(data = roads, lwd = 0.1) +
    geom_sf(data = rivers, color = "blue", alpha = 0.5, lwd = 1.1) +
    geom_sf(data = residential_zones, aes(fill = grade), alpha = 0.5) +
    theme_tufte() +
    scale_fill_manual(values = colors) +
    labs(fill = 'HOLC Categories') +
    theme(
      plot.background = element_rect(fill = "white", color = NA),
      panel.background = element_rect(fill = "white", color = NA),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      legend.position = "right"
    )
  
  # Save the plot as a high-resolution PNG file
  ggsave(filename, plot, width = 10, height = 8, units = "in", dpi = 600)
  
  # Return the plot object if needed for further manipulation or checking
  return(plot)
}
```

```{r, collapse=TRUE}
split_plot <- function(sf_data, roads, rivers) {
  # Filter for grades A, B, C, and D
  sf_data_filtered <- sf_data %>% 
    filter(grade %in% c('A', 'B', 'C', 'D'))

  # Define a color for each grade
  grade_colors <- c("A" = "#76a865", "B" = "#7cb5bd", "C" = "#ffff00", "D" = "#d9838d")

  # Create the plot with panels for each grade
  plot <- ggplot(data = sf_data_filtered) +
    geom_sf(data = roads, alpha = 0.1, lwd = 0.1) +
    geom_sf(data = rivers, color = "blue", alpha = 0.1, lwd = 1.1) +
    geom_sf(aes(fill = grade)) +
    facet_wrap(~ grade, nrow = 1) +  # Free scales for different zoom levels if needed
    scale_fill_manual(values = grade_colors) +
    theme_minimal() +
    labs(fill = 'HOLC Grade') +
    theme_tufte() +
    theme(plot.background = element_rect(fill = "white", color = NA),
          panel.background = element_rect(fill = "white", color = NA),
          legend.position = "none",  # Optionally hide the legend
          axis.text = element_blank(),     # Remove axis text
          axis.title = element_blank(),    # Remove axis titles
          axis.ticks = element_blank(),    # Remove axis ticks
          panel.grid.major = element_blank(),  # Remove major grid lines
          panel.grid.minor = element_blank())  

  ggsave(plot, filename = "HOLC_grades_individually.png", width = 10, height = 4, units = "in", dpi = 1200)
  return(plot)
}
```

```{r, cache=TRUE, warning=FALSE, message=FALSE}
denver_plot <- plot_city_redlining(denver_redlining)
denver_plot
```


```{r, cache=TRUE, warning=FALSE, message=FALSE}
plot_row <- split_plot(denver_redlining, roads, rivers)
plot_row
```

```{r, collapse=TRUE}
process_and_plot_sf_layers <- function(layer1, layer2, output_file = "output_plot.png") {
 # Make geometries valid
layer1 <- st_make_valid(layer1)
layer2 <- st_make_valid(layer2)

# Optionally, simplify geometries to remove duplicate vertices
layer1 <- st_simplify(layer1, preserveTopology = TRUE) |>
  filter(grade != "")

# Prepare a list to store results
results <- list()

# Loop through each grade and perform operations
for (grade in c("A", "B", "C", "D")) {
  # Filter layer1 for current grade
  layer1_grade <- layer1[layer1$grade == grade, ]

  # Buffer the geometries of the current grade
  buffered_layer1_grade <- st_buffer(layer1_grade, dist = 500)

  # Intersect with the second layer
  intersections <- st_intersects(layer2, buffered_layer1_grade, sparse = FALSE)
  selected_polygons <- layer2[rowSums(intersections) > 0, ]

  # Add a new column to store the grade information
  selected_polygons$grade <- grade

  # Store the result
  results[[grade]] <- selected_polygons
}

# Combine all selected polygons from different grades into one sf object
final_selected_polygons <- do.call(rbind, results)

  # Define colors for the grades
  grade_colors <- c("A" = "grey", "B" = "grey", "C" = "grey", "D" = "grey")

  # Create the plot
  plot <- ggplot() +
    geom_sf(data = roads, alpha = 0.05, lwd = 0.1) +
    geom_sf(data = rivers, color = "blue", alpha = 0.1, lwd = 1.1) +
    geom_sf(data = layer1, fill = "grey", color = "grey", size = 0.1) +
    facet_wrap(~ grade, nrow = 1) +
    geom_sf(data = final_selected_polygons,fill = "green", color = "green", size = 0.1) +
    facet_wrap(~ grade, nrow = 1) +
    #scale_fill_manual(values = grade_colors) +
    #scale_color_manual(values = grade_colors) +
    theme_minimal() +
    labs(fill = 'HOLC Grade') +
    theme_tufte() +
    theme(plot.background = element_rect(fill = "white", color = NA),
      panel.background = element_rect(fill = "white", color = NA),
      legend.position = "none",
          axis.text = element_blank(),
          axis.title = element_blank(),
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  # Save the plot as a high-resolution PNG file
  ggsave(output_file, plot, width = 10, height = 4, units = "in", dpi = 1200)
  
  # Return the plot for optional further use
  return(list(plot=plot, sf = final_selected_polygons))
}
```

# NDVI
This section pertain to the processing of satellite imagery to calculate the Normalized Difference Vegetation Index (NDVI), a popular remote sensing measurement used to assess the presence and condition of green vegetation. The NDVI process helps in understanding the spatial distribution of vegetation, urban heat effects, and environmental management.

How to Use the NDVI Functions
Process Satellite Data (process_satellite_data): This function takes a polygon layer (e.g., administrative boundaries or any spatial delineation in sf format), a start and end date, and specific satellite assets (e.g., bands of Sentinel-2). It calculates the NDVI for the specified area and period, creating an animated GIF to visually represent changes over time. This is useful for observing phenological changes or assessing vegetation health periodically.

Yearly Average NDVI (yearly_average_ndvi): This function calculates the yearly average NDVI for a given spatial extent defined by a polygon layer. It filters cloud-free satellite images within a year to compute a median NDVI, providing insights into the annual vegetation status which is crucial for environmental monitoring and urban planning.

Create Mask and Plot (create_mask_and_plot): After processing NDVI, this function overlays the NDVI data on a map with additional geographical layers (e.g., roads, rivers). It applies a mask to segment the NDVI results by different grades or zones within the area, which is particularly useful for detailed spatial analysis in urban planning or environmental studies.





```{r, collapse=TRUE}
create_mask_and_plot <- function(redlining_sf, background_raster = ndvi$raster, roads = NULL, rivers = NULL){
  start_time <- Sys.time()  # Start timing
  
  # Validate and prepare the redlining data
  redlining_sf <- redlining_sf %>%
    filter(grade != "") %>%
    st_make_valid()
  
  
bbox <- st_bbox(redlining_sf)  # Get original bounding box


expanded_bbox <- expand_bbox(bbox, 6000, 1000)  # 

   
expanded_bbox_poly <- st_as_sfc(expanded_bbox, crs = st_crs(redlining_sf)) %>%
    st_make_valid()
  
  # Initialize an empty list to store masks
  masks <- list()
  
  # Iterate over each grade to create masks
  unique_grades <- unique(redlining_sf$grade)
  for (grade in unique_grades) {
    # Filter polygons by grade
    grade_polygons <- redlining_sf[redlining_sf$grade == grade, ]
    
    # Create an "inverted" mask by subtracting these polygons from the background
    mask <- st_difference(expanded_bbox_poly, st_union(grade_polygons))
    
    # Store the mask in the list with the grade as the name
    masks[[grade]] <- st_sf(geometry = mask, grade = grade)
  }
  
  # Combine all masks into a single sf object
  mask_sf <- do.call(rbind, masks)
  
  # Normalize the grades so that C.2 becomes C, but correctly handle other grades
  mask_sf$grade <- ifelse(mask_sf$grade == "C.2", "C", mask_sf$grade)

  # Prepare the plot
  plot <- ggplot() +
    geom_spatraster(data = background_raster, aes(fill = NDVI)) +
  scale_fill_viridis_c(name = "NDVI", option = "viridis", direction = -1) +
   
    geom_sf(data = mask_sf, aes(color = grade), fill = "white", size = 0.1, show.legend = FALSE) +
    scale_color_manual(values = c("A" = "white", "B" = "white", "C" = "white", "D" = "white"), name = "Grade") +
    facet_wrap(~ grade, nrow = 1) +
     geom_sf(data = roads, alpha = 1, lwd = 0.1, color="white") +
    geom_sf(data = rivers, color = "white", alpha = 0.5, lwd = 1.1) +
    labs(title = "NDVI: Normalized Difference Vegetation Index") +
    theme_minimal() +
    coord_sf(xlim = c(bbox["xmin"], bbox["xmax"]), 
           ylim = c(bbox["ymin"], bbox["ymax"]), 
           expand = FALSE) + 
    theme(plot.background = element_rect(fill = "white", color = NA),
          panel.background = element_rect(fill = "white", color = NA),
          legend.position = "bottom",
          axis.text = element_blank(),
          axis.title = element_blank(),
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  # Save the plot
  ggsave("redlining_mask_ndvi.png", plot, width = 10, height = 4, dpi = 600)

  end_time <- Sys.time()  # End timing
  runtime <- end_time - start_time

  # Return the plot and runtime
  return(list(plot = plot, runtime = runtime, mask_sf = mask_sf))
}
```

```{r, collapse=TRUE}
yearly_average_ndvi <- function(polygon_layer, output_file = "ndvi.png", dx = 0.01, dy = 0.01) {
  # Record start time
  start_time <- Sys.time()

  # Calculate the bbox from the polygon layer
  bbox <- st_bbox(polygon_layer)
  
  s = stac("https://earth-search.aws.element84.com/v0")

  # Search for Sentinel-2 images within the bbox for June
  items <- s |> stac_search(
    collections = "sentinel-s2-l2a-cogs",
    bbox = c(bbox["xmin"], bbox["ymin"], bbox["xmax"], bbox["ymax"]),
    datetime = "2023-01-01/2023-12-31",
    limit = 500
  ) %>% 
  post_request()
  
  # Create a collection of images filtering by cloud cover
  col <- stac_image_collection(items$features, asset_names = c("B04", "B08"), property_filter = function(x) {x[["eo:cloud_cover"]] < 80})
  
  # Define a view for processing the data specifically for June
  v <- cube_view(srs = "EPSG:4326", 
                 extent = list(t0 = "2023-01-01", t1 = "2023-12-31",
                               left = bbox["xmin"], right = bbox["xmax"], 
                               top = bbox["ymax"], bottom = bbox["ymin"]),
                 dx = dx, dy = dy, dt = "P1Y", 
                 aggregation = "median", resampling = "bilinear")

  # Process NDVI
  ndvi_rast <- raster_cube(col, v) %>%
    select_bands(c("B04", "B08")) %>%
    apply_pixel("(B08-B04)/(B08+B04)", "NDVI") %>%
    write_tif() |>
    terra::rast()
  
 
  # Convert terra Raster to ggplot using tidyterra
ndvi_plot <-   ggplot() +
    geom_spatraster(data = ndvi_rast, aes(fill = NDVI)) +
    scale_fill_viridis_c(option = "viridis", direction = -1, name = "NDVI") +
    labs(title = "NDVI mean for 2023") +
    theme_minimal() +
    coord_sf() +
    theme(plot.background = element_rect(fill = "white", color = NA),
      panel.background = element_rect(fill = "white", color = NA),
      legend.position = "right",
          axis.text = element_blank(),
          axis.title = element_blank(),
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) 

  # Save the plot as a high-resolution PNG file
  ggsave(output_file, ndvi_plot, width = 10, height = 8, dpi = 600)

  # Calculate processing time
  end_time <- Sys.time()
  processing_time <- difftime(end_time, start_time)

  # Return the plot and processing time
  return(list(plot = ndvi_plot, processing_time = processing_time, raster = ndvi_rast))
}
```

```{r, cache=TRUE}
ndvi_background_low <- yearly_average_ndvi(denver_redlining, dx = 0.01, dy = 0.01)
ndvi_background_low$plot
```


```{r, cache=TRUE}
ndvi <- create_mask_and_plot(denver_redlining, background_raster = ndvi_background_low$raster, roads = roads, rivers = rivers)
ndvi$plot
```

# City of Denver Open Data Portal
This section serves as an interface to the City of Denver Open Data Portal for geographic data analysis. It features a functional approach to access, process, and visualize diverse city inventory datasets. Each dataset—ranging from tree density to crime statistics—is available through direct download and analysis via a centralized function, process_city_inventory_data, which utilizes shapefiles and spatial data frameworks to generate insightful visualizations.

Overview and Usage Instructions:
Function Setup (process_city_inventory_data): This function automates the downloading and reading of shapefiles from specified URLs, processes them according to the geographic area provided (polygon layer), and then plots density maps. These maps can be used to assess various urban factors like tree density or crime rates within specific city zones.

Choice Function (process_city_inventory_data_choice): To streamline user interaction and selection from multiple datasets, this function allows users to choose a dataset by number and pass a spatial polygon for analysis. It maps user input to specific datasets and triggers data processing for that choice.

```{r}
process_city_inventory_data <- function(address, inner_file, polygon_layer, output_filename,variable_label= 'Tree Density') {
  # Download and read the shapefile
  full_path <- glue("/vsizip/vsicurl/{address}/{inner_file}")
  shape_data <- st_read(full_path, quiet = TRUE) |> st_as_sf()

  # Process the shape data with the provided polygon layer
  processed_data <- process_and_plot_sf_layers(polygon_layer, shape_data, paste0(output_filename, ".png"))

  # Extract trees from the processed data
  trees <- processed_data$sf
  denver_redlining_residential <- polygon_layer |> filter(grade != "")

  # Generate the density plot
  plot <- ggplot() +
    geom_sf(data = roads, alpha = 0.05, lwd = 0.1) +
    geom_sf(data = rivers, color = "blue", alpha = 0.1, lwd = 1.1) +
    geom_sf(data = denver_redlining_residential, fill = "grey", color = "grey", size = 0.1) +
    facet_wrap(~ grade, nrow = 1) +
    stat_density_2d(data = trees, 
                    mapping = aes(x = map_dbl(geometry, ~.[1]),
                                  y = map_dbl(geometry, ~.[2]),
                                  fill = stat(density)),
                    geom = 'tile',
                    contour = FALSE,
                    alpha = 0.9) +
    scale_fill_gradientn(colors = c("transparent", "white", "limegreen"),
                         values = scales::rescale(c(0, 0.1, 1)),  # Adjust these based on your density range
                         guide = "colourbar") +
    theme_minimal() +
    labs(fill = variable_label) +
    theme_tufte() +
    theme(plot.background = element_rect(fill = "white", color = NA),
          panel.background = element_rect(fill = "white", color = NA),
          legend.position = "bottom",
          axis.text = element_blank(),
          axis.title = element_blank(),
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  # Save the plot
  ggsave(paste0(output_filename, "_density_plot.png"), plot, width = 10, height = 4, units = "in", dpi = 600)

  # Return the plot and the tree layer
  return(list(plot = plot, layer = trees))
}
```



```{r}
process_city_inventory_data_choice <- function(choice, polygon_layer) {
  # Define the dataset choices
  datasets <- list(
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/tree_inventory/shape/tree_inventory.zip",
         inner_file = "tree_inventory.shp",
         output_filename = "Denver_tree_inventory_2023",
         variable_label = "Tree Density"),
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/traffic_accidents/shape/traffic_accidents.zip",
         inner_file = "traffic_accidents.shp",
         output_filename = "Denver_traffic_accidents",
         variable_label = "Traffic Accidents Density"),
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/instream_sampling_sites/shape/instream_sampling_sites.zip",
         inner_file = "instream_sampling_sites.shp",
         output_filename = "instream_sampling_sites",
         variable_label = "Instream Sampling Sites Density"),
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/soil_samples/shape/soil_samples.zip",
         inner_file = "soil_samples.shp",
         output_filename = "Soil_samples",
         variable_label = "Soil Samples Density"),
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/public_art/shape/public_art.zip",
         inner_file = "public_art.shp",
         output_filename = "Public_art",
         variable_label = "Public Art Density"),
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/liquor_licenses/shape/liquor_licenses.zip",
         inner_file = "liquor_licenses.shp",
         output_filename = "liquor_licenses",
         variable_label = "Liquor Licenses Density"),
    list(address = "https://www.denvergov.org/media/gis/DataCatalog/crime/shape/crime.zip",
         inner_file = "crime.shp",
         output_filename = "Crime",
         variable_label = "Crime Density")
  )

  # Validate input
  if (choice < 1 || choice > length(datasets)) {
    stop("Invalid choice. Please enter a number between 1 and 7.")
  }

  # Get the selected dataset information
  dataset <- datasets[[choice]]

  # Call the original function
  result <- process_city_inventory_data(
    address = dataset$address,
    inner_file = dataset$inner_file,
    polygon_layer = polygon_layer,
    output_filename = dataset$output_filename,
    variable_label = dataset$variable_label
  )
  
  return(result)
}

```


The function process_city_inventory_data_choice allows users to select from a predefined set of datasets for processing. It takes two arguments: choice, an integer that specifies the dataset to process, and polygon_layer, an sf object that represents the geographic area to be analyzed. The choice argument should be a number between 1 and 7, each corresponding to different types of city data:

1 = **Tree Density** - Tree inventory data.
2 = **Traffic Accidents Density** - Traffic accidents data.
3 = **Instream Sampling Sites Density** - Environmental sampling sites data.
4 = **Soil Samples Density** - Soil sample data.
5 = **Public Art Density** - Public art locations.
6 = **Liquor Licenses Density** - Liquor license data.
7 = **Crime Density** - City crime data.

To use this function, simply specify the choice of data and the geographic area as an sf object. The function will process the selected data and return a list containing the generated plot and other relevant data layers. This allows for easy integration and analysis of various city data layers based on spatial parameters.

```{r}
# Example usage:
result <- process_city_inventory_data_choice(1, denver_redlining)
print(result$plot)  # To display the generated plot

```




